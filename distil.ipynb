{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets peft bitsandbytes\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom datasets import load_dataset\nfrom peft import get_peft_model, LoraConfig, TaskType\nimport torch.nn.functional as F\n\nteacher_model_name = \"meta-llama/Llama-3.1-8B\"\nstudent_model_name = \"meta-llama/Llama-3.2-1B\"\n\ntokenizer = AutoTokenizer.from_pretrained(student_model_name, use_fast=True)\n\ndataset = load_dataset(\"hendrycks/ethics\", \"commonsense_train\")\nprint(\"Dataset columns:\", dataset[\"train\"].column_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_function(examples):\n    if \"scenario\" in examples:\n        texts = examples[\"scenario\"]\n    elif \"prompt\" in examples:\n        texts = examples[\"prompt\"]\n    elif \"question\" in examples:\n        texts = examples[\"question\"]\n    elif \"text\" in examples:\n        texts = examples[\"text\"]\n    else:\n        raise ValueError(\"No valid text field found in the dataset!\")\n    return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=128)\n\ntokenized_datasets = dataset.map(preprocess_function, batched=True)\n\nif \"label\" in tokenized_datasets[\"train\"].column_names:\n    tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n\ntokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\nteacher = AutoModelForSequenceClassification.from_pretrained(\n    teacher_model_name,\n    load_in_8bit=True,\n    device_map=\"auto\",\n)\nteacher.eval()\nfor param in teacher.parameters():\n    param.requires_grad = False\n\nstudent = AutoModelForSequenceClassification.from_pretrained(\n    student_model_name,\n    load_in_8bit=True,\n    device_map=\"auto\",\n)\n\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"v_proj\"],\n)\nstudent = get_peft_model(student, lora_config)\nprint(\"Trainable parameters in student:\")\nstudent.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DistillationTrainer(Trainer):\n    def __init__(self, teacher, alpha=0.5, temperature=2.0, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.teacher = teacher\n        self.alpha = alpha\n        self.temperature = temperature\n        self.teacher.eval()\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get(\"labels\")\n        outputs = model(**inputs)\n        student_logits = outputs.logits\n        with torch.no_grad():\n            teacher_outputs = self.teacher(**inputs)\n            teacher_logits = teacher_outputs.logits\n        ce_loss = F.cross_entropy(student_logits, labels)\n        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=-1)\n        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1)\n        kl_loss = F.kl_div(student_log_probs, teacher_probs, reduction=\"batchmean\") * (self.temperature ** 2)\n        loss = self.alpha * kl_loss + (1 - self.alpha) * ce_loss\n        return (loss, outputs) if return_outputs else loss\n\ntraining_args = TrainingArguments(\n    output_dir=\"./distilled_llama3\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_steps=10,\n    learning_rate=2e-5,\n    fp16=True,\n    report_to=\"none\",\n)\n\ntrainer = DistillationTrainer(\n    teacher=teacher,\n    model=student,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"] if \"validation\" in tokenized_datasets else None,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()\n\ntrainer.save_model(\"./distilled_llama3_final\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}